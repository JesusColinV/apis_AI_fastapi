{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ghhWtJlWV7m6"
      },
      "source": [
        "Red neuronal convolucional para problema de clasificación multiclase con keras"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j53m7mK_1yxb"
      },
      "source": [
        "Utilizamos un dataset de la nube, de google colab"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yhaKLEACN-NH"
      },
      "outputs": [],
      "source": [
        "# Utilizamos un dataset de la nube, https://www.kaggle.com/datasets/puneet6060/intel-image-classification\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "!ls \"/content/drive/My Drive\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kgD05QX3N-Ox"
      },
      "outputs": [],
      "source": [
        "# Important imports\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from matplotlib.image import imread\n",
        "import cv2\n",
        "import random\n",
        "from os import listdir\n",
        "from sklearn.preprocessing import  LabelBinarizer\n",
        "from keras.preprocessing import image\n",
        "from keras.preprocessing.image import img_to_array, array_to_img\n",
        "from keras.optimizers import Adam\n",
        "from PIL import Image\n",
        "from keras.models import Sequential\n",
        "from keras.layers.normalization import BatchNormalization\n",
        "from keras.layers import Conv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense, LeakyReLU\n",
        "from sklearn.model_selection import train_test_split"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hzPXISMUN-Sq"
      },
      "outputs": [],
      "source": [
        "# mostramos las primeras 25 imagenes\n",
        "plt.figure(figsize=(11,11))\n",
        "path = \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset/mountain\"\n",
        "for i in range(1,26):\n",
        "    plt.subplot(5,5,i)\n",
        "    plt.tight_layout()\n",
        "    rand_img = imread(path +'/'+ random.choice(sorted(listdir(path))))\n",
        "    plt.imshow(rand_img)\n",
        "    plt.title('mountain')\n",
        "    plt.xlabel(rand_img.shape[1], fontsize = 10)\n",
        "    plt.ylabel(rand_img.shape[0], fontsize = 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mdyiHpahReF8"
      },
      "outputs": [],
      "source": [
        "# creación de listas en el directorio raiz\n",
        "dir = \"/content/drive/My Drive/Intel Image Dataset/Intel Image Dataset\" \n",
        "root_dir = listdir(dir)\n",
        "image_list, label_list = [], []"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cLoajsN-RmQF"
      },
      "outputs": [],
      "source": [
        "# Convertimos imagenes a numeros (numpy) en listas de imagenes y etiquetas\n",
        "for directory in root_dir:\n",
        "  for files in listdir(f\"{dir}/{directory}\"):\n",
        "    image_path = f\"{dir}/{directory}/{files}\"\n",
        "    image = Image.open(image_path)\n",
        "    image = image.resize((150,150)) # Estandarizamos las dimenciones de las imagenes\n",
        "    image = img_to_array(image)\n",
        "    image_list.append(image)\n",
        "    label_list.append(directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VX89xh5BRmlG"
      },
      "outputs": [],
      "source": [
        "# Numero de clases posibles e imagen por clase\n",
        "label_counts = pd.DataFrame(label_list).value_counts()\n",
        "\n",
        "num_classes = len(label_counts)\n",
        "print(\"clases\",label_counts)\n",
        "print(\"numero por clases\",num_classes)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JBhRcDCf5-eN"
      },
      "source": [
        "Número de clases que se utilizarán más adelante en la arquitectura del modelo  \n",
        "\n",
        "Tamaño para capa de entrada"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zJp5JRT0Rp7e"
      },
      "outputs": [],
      "source": [
        "np.array(image_list).shape\n",
        "label_list = np.array(label_list)\n",
        "label_list.shape"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RhTfmSvoRt5l"
      },
      "outputs": [],
      "source": [
        "# separameos test y train\n",
        "x_train, x_test, y_train, y_test = train_test_split(image_list, label_list, test_size=0.2, random_state = 10) "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P5GWA6z5Rxtm"
      },
      "outputs": [],
      "source": [
        "# Normalizamos\n",
        "# Binarizing labels cambia 255 canales de color a 1 y 0 escala de grises\n",
        "x_train = np.array(x_train, dtype=np.float16) / 225.0\n",
        "x_test = np.array(x_test, dtype=np.float16) / 225.0\n",
        "x_train = x_train.reshape( -1, 150,150,3)\n",
        "x_test = x_test.reshape( -1, 150,150,3)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EuVKsoqF7XK_"
      },
      "source": [
        "One hot enconder para las etiquetas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZ8NTYmfRzOS"
      },
      "outputs": [],
      "source": [
        "lb = LabelBinarizer()\n",
        "y_train = lb.fit_transform(y_train)\n",
        "y_test = lb.fit_transform(y_test)\n",
        "print(lb.classes_)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JshCofPfR01i"
      },
      "outputs": [],
      "source": [
        "# Dividimos el entrenamiento para validación tambien\n",
        "x_train, x_val, y_train, y_val = train_test_split(x_train, y_train, test_size = 0.2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hN8XnsG9R2oA"
      },
      "outputs": [],
      "source": [
        "# Arquitectura del modelo\n",
        "model = Sequential([\n",
        "        Conv2D(16, kernel_size = (3,3), input_shape = (150,150,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "          \n",
        "        Conv2D(32, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "        \n",
        "        Conv2D(64, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Conv2D(128, kernel_size = (3,3)),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        MaxPooling2D(5,5),\n",
        "\n",
        "        Flatten(),\n",
        "    \n",
        "        Dense(64),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "        \n",
        "        Dense(32),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(),\n",
        "    \n",
        "        Dense(16),\n",
        "        Dropout(rate = 0.2),\n",
        "        BatchNormalization(),\n",
        "        LeakyReLU(1),\n",
        "    \n",
        "        Dense(6, activation = 'softmax')    \n",
        "        ])\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HyAv8tUF72vw"
      },
      "source": [
        "Hemos utilizado diferentes tipos de capas según sus características a saber:\n",
        "BatchNormalization \n",
        "(La normalización por lotes es una técnica para el entrenamiento de redes neuronales muy profundas que normaliza las entradas a una capa para cada minilote), \n",
        "\n",
        "LeakyRelu \n",
        "(El Leaky ReLU modifica la función para permitir pequeños valores negativos cuando la entrada es menor que cero), \n",
        "\n",
        "Conv_2d \n",
        "(Se utiliza para crear un kernel convolucional que se convoluciona con la capa de entrada para producir el tensor de salida),\n",
        "\n",
        "max_pooling2d \n",
        "(Es una técnica de downsampling que saca el valor máximo sobre la ventana definida por poolsize), \n",
        "\n",
        "flatten \n",
        "(Aplana la entrada y crea una salida 1D), \n",
        "\n",
        "Dense \n",
        "(La capa Dense produce la salida como el producto punto de la entrada y el kernel). \n",
        "\n",
        "En la última capa utilizaremos softmax como función de activación porque se trata de un problema de clasificación de varias clases."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1GvVgEiXR4sy"
      },
      "outputs": [],
      "source": [
        "# Compilamos el modelo\n",
        "model.compile(loss = 'categorical_crossentropy', optimizer = Adam(0.0005),metrics=['accuracy'])\n",
        "\"\"\"\n",
        "3 parámetros: \n",
        "pérdida, optimizador y métrica. \n",
        "Aquí usaremos la pérdida como categorical_crossentropy, el optimizador como Adam y la métrica como precisión.\n",
        "\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1I9HtY2R7R-"
      },
      "outputs": [],
      "source": [
        "# Entrenamiento\n",
        "epochs = 70\n",
        "batch_size = 128\n",
        "history = model.fit(\n",
        "    x_train, \n",
        "    y_train, \n",
        "    batch_size = batch_size, \n",
        "    epochs = epochs, \n",
        "    validation_data = (x_val, y_val)\n",
        "    )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mxJ2AIYK8y2t"
      },
      "source": [
        "Se puede intentar utilizar un mayor número de épocas para aumentar la precisión. Durante cada época podemos ver cómo se comporta el modelo viendo la precisión del entrenamiento y de la validación."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_ZBnscUwSDi5"
      },
      "outputs": [],
      "source": [
        "# Guardamos el modelo\n",
        "model.save(\"/content/drive/My Drive/intel_image.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pBEz7EGGSFro"
      },
      "outputs": [],
      "source": [
        "#Entrenamiento en el tiempo\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['accuracy'], color='r')\n",
        "plt.plot(history.history['val_accuracy'], color='b')\n",
        "plt.title('Model Accuracy')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "auY2U-1USIk2"
      },
      "outputs": [],
      "source": [
        "#Funcion de perdida\n",
        "plt.figure(figsize=(12, 5))\n",
        "plt.plot(history.history['loss'], color='r')\n",
        "plt.plot(history.history['val_loss'], color='b')\n",
        "plt.title('Model Loss')\n",
        "plt.ylabel('Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.legend(['train', 'val'])\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RRI9dX9eSI0j"
      },
      "outputs": [],
      "source": [
        "scores = model.evaluate(x_test, y_test)\n",
        "print(f\"Test Accuracy: {scores[1]*100}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_931pQbeSKyO"
      },
      "outputs": [],
      "source": [
        "# Evaluacion del modelo\n",
        "y_pred = model.predict(x_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aNByJOaYSMNh"
      },
      "outputs": [],
      "source": [
        "img = array_to_img(x_test[1])\n",
        "img"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YLdmALWF9rYp"
      },
      "source": [
        "visualiamos imagenes original vs predicha"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yqAQkqooSQAo"
      },
      "outputs": [],
      "source": [
        "labels = lb.classes_\n",
        "print(labels)\n",
        "print(\"Originally : \",labels[np.argmax(y_test[1])])\n",
        "print(\"Predicted : \",labels[np.argmax(y_pred[1])])"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Intel Image Classification.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.8.10 ('venv': venv)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "c9907b5e0aaf5ea94a22e71d725213a1d51c12130c171e855b1845aefc822d17"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
